# Applying Thread Pools
* [Sizing Thread Pools](#sizing-thread-pools)
* [Thread Creation and Teardown](#thread-creation-and-teardown)
* [Managing Queued Tasks](#managing-queued-tasks)
* [Saturation Policies](#saturation-policies)
* [Thread Factories](#thread-factories)
* [Customizing ThreadPoolExecutor After Construction](#customizing-threadpoolexecutor-after-construction)
* [Extending ThreadPoolExecutor](#extending-threadpoolexecutor)
Some tasks have characteristics that require or preclude a specific execution policy. Tasks that depend on other tasks require that the thread pool be large enough that tasks are never queued or rejected; tasks that exploit thread confinement require sequential execution.

In a single-threaded executor, a task that submits another task to the same executor and waits for its result will always deadlock. The second task sits on the work queue until the first task completes, but the first will not complete because it is waiting for the result of the second task. The same thing can happen in larger thread pools if all threads are executing tasks that are blocked waiting for other tasks still on the work queue. This is called thread starvation deadlock, and can occur whenever a pool task initiates an unbounded blocking wait for some resource or condition that can succeed only through the action of another pool task, such as waiting for the return value or side effect of another task, unless you can guarantee that the pool is large enough.
#### Sizing Thread Pools
The ideal size for a thread pool depends on the types of tasks that will be submitted and the characteristics of the deployment system. Thread pool sizes should rarely be hard-coded; instead pool sizes should be provided by a configuration mechanism or computed dynamically by consulting **Runtime.availableProcessors**.

For compute-intensive tasks, an **Ncpu-processor** system usually achieves optimum utilization with a thread pool of **Ncpu +1** threads. (Even compute-intensive threads occasionally take a page fault or pause for some other reason, so an "extra" runnable thread prevents CPU cycles from going unused when this happens.) For tasks that also include I/O or other blocking operations, you want a larger pool, since not all of the threads will be schedulable at all times. In order to size the pool properly, you must estimate the ratio of waiting time to compute time for your tasks; this estimate need not be precise and can be obtained through pro-filing or instrumentation. Alternatively, the size of the thread pool can be tuned by running the application using several different pool sizes under a benchmark load and observing the level of CPU utilization.

Of course, CPU cycles are not the only resource you might want to manage using thread pools. Other resources that can contribute to sizing constraints are memory, file handles, socket handles, and database connections. Calculating pool size constraints for these types of resources is easier: just add up how much of that resource each task requires and divide that into the total quantity available. The result will be an upper bound on the pool size. When tasks require a pooled resource such as database connections, thread pool size and resource pool size affect each other. If each task requires a connection, the effective size of the thread pool is limited by the connection pool size. Similarly, when the only consumers of connections are pool tasks, the effective size of the connection pool is limited by the thread pool size.
#### Thread Creation and Teardown 
The **core pool size**, **maximum pool size**, and **keep-alive time** govern thread creation and teardown. The **core size** is the target size; the implementation attempts to maintain the pool at this size even when there are no tasks to execute, and will not create more threads than this unless the work queue is full. The **maximum pool size** is the upper bound on how many pool threads can be active at once. A thread that has been idle for longer than the **keep-alive time** becomes a candidate for reaping and can be terminated if the current pool size exceeds the **core size**.
### Managing Queued Tasks
With a thread pool, tasks wait in a queue of **Runnables** managed by the **Executor** instead of queueing up as threads contending for the CPU.

The default for **newFixedThreadPool** and **newSingleThreadExecutor** is to use an unbounded **LinkedBlockingQueue**. Tasks will queue up if all worker threads are busy, but the queue could grow without bound if the tasks keep arriving faster than they can be executed.

A more stable resource management strategy is to use a bounded queue, such as an **ArrayBlockingQueue** or a bounded **LinkedBlockingQueue** or **Priority-BlockingQueue**. Bounded queues help prevent resource exhaustion but introduce the question of what to do with new tasks when the queue is full. With a bounded work queue, the queue size and pool size must be tuned together. A large queue coupled with a small pool can help reduce memory usage, CPU usage, and context switching, at the cost of potentially constraining throughput.

For very large or unbounded pools, you can also bypass queuing entirely and instead hand off tasks directly from producers to worker threads using a **SynchronousQueue**. A **SynchronousQueue** is not really a queue at all, but a mechanism for managing handoffs between threads. In order to put an element on a **SynchronousQueue**, another thread must already be waiting to accept the handoff. If no thread is waiting but the current pool size is less than the maximum, **ThreadPoolExecutor** creates a new thread; otherwise the task is rejected according to the saturation policy. Using a direct handoff is more efficient because the task can be handed right to the thread that will execute it, rather than first placing it on a queue and then having the worker thread fetch it from the queue. **SynchronousQueue** is a practical choice only if the pool is unbounded or if rejecting excess tasks is acceptable. The **newCachedThreadPool** factory uses a **SynchronousQueue**.

The **newCachedThreadPool** factory is a good default choice for an **Executor**, providing better queuing performance than a fixed thread pool. A fixed size thread pool is a good choice when you need to limit the number of concurrent tasks for resource-management purposes, as in a server application that accepts requests from network clients and would otherwise be vulnerable to overload.

Bounding either the thread pool or the work queue is suitable only when tasks are independent. With tasks that depend 
on other tasks, bounded thread pools or queues can cause thread starvation deadlock; instead, use an unbounded pool 
configuration like **newCachedThreadPool**.
#### Saturation Policies
When a bounded work queue fills up, the saturation policy comes into play. The saturation policy for a **ThreadPoolExecutor** can be modified by calling **setRejectedExecutionHandler**. (The saturation policy is also used when a task is submitted to an **Executor** that has been shut down.) Several implementations of **RejectedExecutionHandler** are provided, each implementing a different saturation policy: **AbortPolicy**, **CallerRunsPolicy**, **DiscardPolicy**, and **DiscardOldestPolicy**.

The default policy, abort, causes execute to throw the unchecked **RejectedExecutionException**; the caller can catch this exception and implement its own overflow handling as it sees fit. The discard policy silently discards the newly submitted task if it cannot be queued for execution; the discard-oldest policy discards the task that would otherwise be executed next and tries to resubmit the new task. (If the work queue is a priority queue, this discards the highest-priority element, so the combination of a discard-oldest saturation policy and a priority queue is not a good one.) The caller-runs policy implements a form of throttling that neither discards tasks nor throws an exception, but instead tries to slow down the flow of new tasks by pushing some of the work back to the caller. It executes the newly submitted task not in a pool thread, but in the thread that calls execute. As the server becomes overloaded, the overload is gradually pushed outward - from the pool threads to the work queue to the application to the TCP layer, and eventually to the client - enabling more graceful degradation under load.
#### Thread Factories
Whenever a thread pool needs to create a thread, it does so through a thread factory. The default thread factory creates a new, nondaemon thread with no special configuration. Specifying a thread factory allows you to customize the configuration of pool threads. **ThreadFactory** has a single method, **newThread**, that is called whenever a thread pool needs to create a new thread.
#### Customizing ThreadPoolExecutor After Construction
Most of the options passed to the **ThreadPoolExecutor** constructors can also be modified after construction via setters (such as the core thread pool size, maximum thread pool size, keep-alive time, thread factory, and rejected execution handler). If the Executor is created through one of the factory methods in **Executors** (except **newSingleThreadExecutor**), you can cast the result to **ThreadPoolExecutor** to access the setters.

Executors includes a factory method, **unconfigurableExecutorService**, which takes an existing **ExecutorService** and wraps it with one exposing only the methods of **ExecutorService** so it cannot be further configured. Unlike the pooled implementations, **newSingleThreadExecutor** returns an **ExecutorService** wrapped in this manner, rather than a raw **ThreadPoolExecutor**.
#### Extending ThreadPoolExecutor 
**ThreadPoolExecutor** was designed for extension, providing several "hooks" for subclasses to override **beforeExecute**, **afterExecute**, and **terminatethat** can be used to extend the behavior of **ThreadPoolExecutor**. The **beforeExecute** and **afterExecute** hooks are called in the thread that executes the task, and can be used for adding logging, timing, monitoring, or statistics gathering. The **afterExecute** hook is called whether the task completes by returning normally from run or by throwing an Exception. (If the task completes with an **Error**, **afterExecute** is not called.) If **beforeExecute** throws a **RuntimeException**, the task is not executed and **afterExecute** is not called. The **terminated** hook is called when the thread pool completes the shutdown process, after all tasks have finished and all worker threads have shut down. It can be used to release resources allocated by the **Executor** during its lifecycle, perform notification or logging, or finalize statistics gathering.