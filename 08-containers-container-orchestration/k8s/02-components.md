# k8s Components

* [Head Node Components](#head-node-components)
* [Components On All Nodes](#components-on-all-nodes)
* [Scheduled Components](#scheduled-components)

Kubernetes is a system that groups together a large fleet of machines into a single unit that can be consumed via an API, but the implementation of Kubernetes actually subdivides the set of machines into two groups: worker nodes and head nodes. Most of the components that make up the Kubernetes infrastructure run on head or control plane nodes. There are a limited number of such nodes in a cluster, generally one, three, or five. These nodes run the components that implement Kubernetes, like etcd and the API server. There is an odd number of these nodes, since they need to keep quorum in a shared state using a Raft/Paxos algorithm for quorum. The cluster’s actual work is done on the worker nodes. These nodes also run a more limited selection of Kubernetes components. Finally, there are Kubernetes components that are scheduled to the Kubernetes cluster after it is created. From a Kubernetes perspective, these components are indistinguishable from other workloads, but they do implement part of the overall Kubernetes API.

### Head Node Components
#### etcd
The etcd system is at the heart of any Kubernetes cluster. It implements the key-value stores where all of the objects in a Kubernetes cluster are persisted. The etcd servers implement a distributed consensus algorithm, namely Raft, which ensures that even if one of the storage servers fails, there is sufficient replication to maintain the data stored in etcd and to recover data when an etcd server becomes healthy again and readds itself to the cluster. The etcd servers also provide two other important pieces of functionality that Kubernetes makes heavy use of. The first is optimistic concurrency. Every value stored in etcd has a corresponding resource version. When a key-value pair is written to an etcd server, it can be conditionalized on a particular resource version. This means that, using etcd, you can implement compare and swap, which is at the core of any concurrency system. Compare and swap enables a user to read a value and update it knowing that no other component in the system has also updated the value. These assurances enable the system to safely have multiple threads manipulating data in etcd without the need for pessimistic locks, which can significantly reduce throughput to the server.

In addition to implementing compare and swap, the etcd servers also implement a watch protocol. The value of watch is that it enables clients to efficiently watch for changes in the key-value stores for an entire directory of values. As an example, all objects in a Namespace are stored within a directory in etcd. The use of a watch enables a client to efficiently wait for and react to changes without continuous polling of the etcd server.

#### API server
Although etcd is at the core of a Kubernetes cluster, there is actually only a single server that is allowed to have direct access to the Kubernetes cluster, and that is the API server. The API server is the hub of the Kubernetes cluster; it mediates all interactions between clients and the API objects stored in etcd. Consequently, it is the central meeting point for all of the various components.

#### Scheduler
With etcd and the API server operating correctly, a Kubernetes cluster is, in some ways, functionally complete. You can create all of the different API objects, like Deployments and Pods. However, you will find that it never begins to run. Finding a location for a Pod to run is the job of the Kubernetes scheduler. The scheduler scans the API server for unscheduled Pods and then determines the best nodes on which to run them.

#### Controller manager
After etcd, the API server, and the scheduler are operational, you can successfully create Pods and see them scheduled out onto nodes, but you will find that ReplicaSets, Deployments, and Services don’t work as you expect them to. This is because all of the reconciliation control loops needed to implement this functionality are not currently running. Executing these loops is the job of the controller manager. The controller manager is the most varied of all of the Kubernetes components, since it has within it numerous different reconciliation control loops to implement many parts of the overall Kubernetes system.

### Components On All Nodes
#### Kubelet
The kubelet is the node daemon for all machines that are part of a Kubernetes cluster. The kubelet is the bridge that joins the available CPU, disk, and memory for a node into the large Kubernetes cluster. The kubelet communicates with the API server to find containers that should be running on its node. Likewise, the kubelet communicates the state of these containers back up to the API server so that other reconciliation control loops can observe the current state of these containers. 

In addition to scheduling and reporting the state of containers running in Pods on their machines, kubelets are also responsible for health checking and restarting the containers that are supposed to be executing on their machines. It would be quite inefficient to push all of the health-state information back up to the API server so that reconciliation loops can take action to fix the health of a container on a particular machine. Instead, the kubelet shortcircuits this interaction and runs the reconciliation loop itself. Thus, if a container being run by the kubelet dies or fails its health check, the kubelet restarts it, while also communicating this health state (and the restart) back up to the API server.

#### kube-proxy
The other component that runs on all machines is the kube-proxy. The kube-proxy is responsible for implementing the Kubernetes Service load-balancer networking model. The kube-proxy is always watching the endpoint objects for all Services in the Kubernetes cluster. The kube-proxy then programs the network on its node so that network requests to the virtual IP address of a Service are, in fact, routed to the endpoints that implement this Service. Every Service in Kubernetes gets a virtual IP address, and the kube-proxy is the daemon responsible for defining and implementing the local load balancer that routes traffic from Pods on the machine to Pods, anywhere in the cluster, that implement the Service.

### Scheduled Components
When all of the components just described are successfully operating, they provide a minimally viable Kubernetes cluster. But there are several additional scheduled components that are essential to the Kubernetes cluster that actually rely on the cluster itself for their implementation. This means that, although they are essential to cluster function, they also are scheduled, health checked, operated, and updated using calls to the Kubernetes API server itself.

#### KubeDNS
The first of these scheduled components is the KubeDNS server. When a Kubernetes Service is created, it gets a virtual IP address, but that IP address is also programmed into a DNS server for easy service discovery. The KubeDNS containers implement this name-service for Kubernetes Service objects. The KubeDNS Service is itself expressed as a Kubernetes Service, so the same routing provided by the kube-proxy routes DNS traffic to the KubeDNS containers. The one important difference is that the KubeDNS service is given a static virtual IP address. This means that the API server can program the DNS server into all of the containers that it creates, implementing the naming and service discovery for Kubernetes services.

#### Heapster
The other scheduled component is a binary called Heapster, which is responsible for collecting metrics like CPU, network, and disk usage from all containers running inside the Kubernetes cluster. These metrics can be pushed to a monitoring system, like InfluxDB, for alerting and general monitoring of application health in the cluster. Also, importantly, these metrics are used to implement autoscaling of Pods within the Kubernetes cluster. Kubernetes has an autoscaler implementation, that, for example, can automatically scale the size of a Deployment whenever the CPU usage of the containers in the Deployment goes above 80%. Heapster is the component that collects and aggregates these metrics to power the reconciliation loop implemented by the autoscaler. The autoscaler observes the current state of the world through API calls to Heapster.

#### Add-ons
In addition to these core components, there are numerous systems that you can find on most installations of Kubernetes. These include the Kubernetes dashboard, as well as community add-ons, like functions as a service (FaaS), automatic certificate agents, and many more.